# @package _group_
task:
  _name: translation_lev
  data: /home/neil/nanmt-fairseq/iwslt14.tokenized.de-en # path to data directory
  source_lang: de
  target_lang: en
  noise: random_mask
criterion:
  _name: rl_loss
  sentence_level_metric: comet # one of bleu, chrf, meteor, rouge, ter, bertscore, bleurt, comet
  sampling: multinomial
  # wandb_entity: abhinav-neil
  # use_wandb: true
  wandb_project: nlp2-nanmt
  wandb_run: comet_1
model:
  _name: cmlm_transformer_base
  share_decoder_input_output_embed: true
  decoder:
    learned_pos: true
  encoder:
    learned_pos: true
  dropout: 0.2
  label_smoothing: 0.1
  length_loss_factor: 0.01
optimizer:
  _name: adam
  adam_betas: (0.9,0.98)
lr_scheduler:
  _name: inverse_sqrt
  warmup_updates: 1
  warmup_init_lr: 1e-07
dataset:
  batch_size: 64
  # max_tokens: 8192
  batch_size_valid: 16
  # validate_interval: 999
  # validate_interval_updates: -1
  disable_validation: true
optimization:
  lr: [0.0001]
  update_freq: [8] # update every N batches
  max_update: 1000  # max # updates
  max_epoch: 0  # max # training epochs
  stop_min_lr: 1e-09
checkpoint:
  save_dir: /home/neil/nanmt-fairseq/fairseq_easy_extend/models/nat/checkpoints/comet/
  restore_file: /home/neil/nanmt-fairseq/fairseq_easy_extend/models/nat/checkpoints/checkpoint_pretrained.pt
  # finetune_from_model: true
  reset_optimizer: true
  save_interval: 1  # save a checkpoint every N epochs
  save_interval_updates: 20  # save a checkpoint every N updates
  keep_interval_updates: 0  # keep last N checkpoints
  keep_best_checkpoints: 1 # keep best N checkpoints
  no_epoch_checkpoints: true
  no_last_checkpoints: true
  patience: -1 # early stopping in N validations
  checkpoint_suffix: '_1' # suffix to add to the checkpoint file name
common:
  log_format: simple
  log_interval: 100